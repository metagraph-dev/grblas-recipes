{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88285a34",
   "metadata": {},
   "source": [
    "# GraphSAGE inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a183732d-284e-4752-86af-0f97bc482b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from grblas import *\n",
    "import grblas as gb\n",
    "import numpy as np\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5bba7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array(\n",
    "    [\n",
    "        [0, 1, 0, 1, 1, 0, 0, 0],\n",
    "        [1, 0, 0, 1, 1, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 1, 1, 1, 1],\n",
    "        [1, 1, 0, 0, 1, 0, 0, 0],\n",
    "        [1, 1, 1, 1, 0, 0, 0, 0],\n",
    "        [0, 0, 1, 0, 0, 0, 1, 0],\n",
    "        [0, 0, 1, 0, 0, 1, 0, 0],\n",
    "        [0, 0, 1, 0, 0, 0, 0, 0],\n",
    "    ]\n",
    ")\n",
    "A = gb.io.from_numpy(a)\n",
    "A = ss.concat([[A, A], [A, A]])\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b174bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "nnodes = A.nrows\n",
    "ndims = [7, 11, 13]\n",
    "nsamples = [2, 3]\n",
    "assert len(ndims) == len(nsamples) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3569d9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's randomly create features and weights\n",
    "features = Matrix.ss.import_fullr(np.random.rand(nnodes, ndims[0]) - 0.4)\n",
    "weights = []\n",
    "prev_dim = ndims[0]\n",
    "for ndim in ndims[1:]:\n",
    "    weights.append(Matrix.ss.import_fullr(np.random.rand(2*prev_dim, ndim) - 0.4))\n",
    "    prev_dim = ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5ee1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We could easily have a bias term.  Should we?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9a7078",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('A.shape:', A.shape)\n",
    "print('features.shape:', features.shape)\n",
    "for i, W in enumerate(weights):\n",
    "    print(f'weights[{i}].shape:', W.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11691a05",
   "metadata": {},
   "source": [
    "### Algorithm 1 from https://arxiv.org/pdf/1706.02216.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26243f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = features\n",
    "for k in range(len(nsamples)):\n",
    "    # Randomly select neighborhoods\n",
    "    neighborhoods = A.ss.selectk_rowwise(\"random\", nsamples[k])\n",
    "    next_h = Matrix.new(h.dtype, nrows=nnodes, ncols=ndims[k + 1])\n",
    "    for v in range(nnodes):\n",
    "        # Sample neighborhood\n",
    "        neighborhood = ss.diag(neighborhoods[v, :].new())\n",
    "        # Aggregate neighborhood h vectors\n",
    "        neighborhood_hs = op.any_second(neighborhood @ h).new()\n",
    "        neighbor_aggregated = neighborhood_hs.reduce_columnwise(agg.mean).new()\n",
    "        # Concat h vectors together (a Vector concat function may be handy!)\n",
    "        # We could also split the weight matrices in two and either concat the results\n",
    "        # or keep the results separate until the very, very end.\n",
    "        size = neighbor_aggregated.size\n",
    "        h_concat = Vector.new(h.dtype, size=2*size)\n",
    "        h_concat[:size] = h[v, :].new()\n",
    "        h_concat[size:] = neighbor_aggregated\n",
    "        # Apply weight matrix\n",
    "        val = op.plus_times(h_concat @ weights[k]).new()\n",
    "        # Could apply bias term here\n",
    "        # Apply ReLU\n",
    "        val = op.max(val, 0).new()  # We could make this sparse by using select with GT_ZERO\n",
    "        # Normalize (could also do this on next_h all at once)\n",
    "        denom = val.reduce(agg.L2norm).new()  # L2norm is same as hypot monoid: (x**2 + y**2)**0.5\n",
    "        val = binary.truediv(val, denom).new()\n",
    "        # A function to stack Vectors into a Matrix may be handy! (as would dense arrays)\n",
    "        next_h[v, :] = val\n",
    "    h = next_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937985cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30edb5c",
   "metadata": {},
   "source": [
    "### Algorithm 2 from https://arxiv.org/pdf/1706.02216.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b92dbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The batch we're interested in (could be any number of nodes)\n",
    "batch = Vector.new(A.dtype, size=nnodes)\n",
    "batch[0] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b23eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bs = [None for _ in range(len(nsamples))]\n",
    "Bs.append(batch)\n",
    "neighborhoods = [None for _ in range(len(nsamples))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e974819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute upfront the neighborhoods for the nodes we'll need\n",
    "for k in range(len(nsamples), 0, -1):\n",
    "    B = Bs[k]\n",
    "    D = ss.diag(B)\n",
    "    subset = op.any_second(D @ A).new()\n",
    "    neighborhoods[k - 1] = subset.ss.selectk_rowwise('random', nsamples[k - 1])\n",
    "    nodes = neighborhoods[k - 1].reduce_columnwise(op.any).new()\n",
    "    Bs[k - 1] = op.any(B | nodes).new()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cee3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = op.any_second(ss.diag(Bs[0]) @ features).new()\n",
    "for k in range(len(nsamples)):\n",
    "    # Randomly select neighborhoods\n",
    "    next_h = Matrix.new(h.dtype, nrows=nnodes, ncols=ndims[k + 1])\n",
    "    B = Bs[k + 1]\n",
    "    indices, _ = B.to_values()\n",
    "    for v in indices:\n",
    "        # Sample neighborhood\n",
    "        neighborhood = ss.diag(neighborhoods[k][v, :].new())\n",
    "        \n",
    "        # The rest is the same as above in algorithm 1\n",
    "        \n",
    "        # Aggregate neighborhood h vectors\n",
    "        neighborhood_hs = op.any_second(neighborhood @ h).new()\n",
    "        neighbor_aggregated = neighborhood_hs.reduce_columnwise(agg.mean).new()\n",
    "        # Concat h vectors together (a Vector concat function may be handy!)\n",
    "        # We could also split the weight matrices in two and either concat the results\n",
    "        # or keep the results separate until the very, very end.\n",
    "        size = neighbor_aggregated.size\n",
    "        h_concat = Vector.new(h.dtype, size=2*size)\n",
    "        h_concat[:size] = h[v, :].new()\n",
    "        h_concat[size:] = neighbor_aggregated\n",
    "        # Apply weight matrix\n",
    "        val = op.plus_times(h_concat @ weights[k]).new()\n",
    "        # Could apply bias term here\n",
    "        # Apply ReLU\n",
    "        val = op.max(val, 0).new()  # We could make this sparse by using select with GT_ZERO\n",
    "        # Normalize (could also do this on next_h all at once)\n",
    "        denom = val.reduce(agg.L2norm).new()  # L2norm is same as hypot monoid: (x**2 + y**2)**0.5\n",
    "        val = binary.truediv(val, denom).new()\n",
    "        # A function to stack Vectors into a Matrix may be handy! (as would dense arrays)\n",
    "        next_h[v, :] = val\n",
    "    h = next_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cf7d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "h"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
